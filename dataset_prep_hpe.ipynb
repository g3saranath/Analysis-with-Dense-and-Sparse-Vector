{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial Stock Price Dataset: Daily or intraday data for various stocks, which can be obtained from financial databases like Yahoo Finance or Alpha Vantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
      "Date                                                                          \n",
      "2023-01-03  16.090000  16.139999  15.830000  16.059999  15.367215  18233900   \n",
      "2023-01-04  16.150000  16.525000  16.120001  16.420000  15.711686  15865500   \n",
      "2023-01-05  16.290001  16.495001  16.184999  16.450001  15.740394  13099500   \n",
      "2023-01-06  16.680000  17.209999  16.670000  17.110001  16.371922  15242500   \n",
      "2023-01-09  17.100000  17.250000  16.889999  16.959999  16.228392  19294700   \n",
      "\n",
      "                               Company  \n",
      "Date                                    \n",
      "2023-01-03  Hewlett Packard Enterprise  \n",
      "2023-01-04  Hewlett Packard Enterprise  \n",
      "2023-01-05  Hewlett Packard Enterprise  \n",
      "2023-01-06  Hewlett Packard Enterprise  \n",
      "2023-01-09  Hewlett Packard Enterprise  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Fetching historical stock prices from Yahoo Finance\n",
    "import yfinance as yf\n",
    "\n",
    "# Define the stock ticker and time period\n",
    "ticker = 'HPE'\n",
    "data = yf.download(ticker, start='2023-01-01', end='2024-05-01')\n",
    "data[\"Company\"] = \"Hewlett Packard Enterprise\"\n",
    "\n",
    "# ticker = 'MSFT'\n",
    "# data2 = yf.download(ticker, start='2023-01-01', end='2024-05-01')\n",
    "# data2[\"Company\"] = \"Microsoft\"\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/HPE_stock.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPE Stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>3.330000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.220631</td>\n",
       "      <td>16.418802</td>\n",
       "      <td>16.045859</td>\n",
       "      <td>16.234535</td>\n",
       "      <td>15.830550</td>\n",
       "      <td>1.267898e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.090092</td>\n",
       "      <td>1.123950</td>\n",
       "      <td>1.070711</td>\n",
       "      <td>1.085906</td>\n",
       "      <td>1.145174</td>\n",
       "      <td>6.267215e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.700000</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>13.655000</td>\n",
       "      <td>13.780000</td>\n",
       "      <td>13.300308</td>\n",
       "      <td>3.788800e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.450000</td>\n",
       "      <td>15.620000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>15.077985</td>\n",
       "      <td>8.779800e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.309999</td>\n",
       "      <td>16.495001</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>16.389999</td>\n",
       "      <td>15.826511</td>\n",
       "      <td>1.098290e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.030001</td>\n",
       "      <td>17.245001</td>\n",
       "      <td>16.875000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.751013</td>\n",
       "      <td>1.461540e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.790001</td>\n",
       "      <td>20.070000</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>18.521847</td>\n",
       "      <td>5.125090e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open        High         Low       Close   Adj Close  \\\n",
       "count  333.000000  333.000000  333.000000  333.000000  333.000000   \n",
       "mean    16.220631   16.418802   16.045859   16.234535   15.830550   \n",
       "std      1.090092    1.123950    1.070711    1.085906    1.145174   \n",
       "min     13.700000   13.900000   13.655000   13.780000   13.300308   \n",
       "25%     15.450000   15.620000   15.260000   15.500000   15.077985   \n",
       "50%     16.309999   16.495001   16.180000   16.389999   15.826511   \n",
       "75%     17.030001   17.245001   16.875000   17.049999   16.751013   \n",
       "max     18.790001   20.070000   18.340000   18.770000   18.521847   \n",
       "\n",
       "             Volume  \n",
       "count  3.330000e+02  \n",
       "mean   1.267898e+07  \n",
       "std    6.267215e+06  \n",
       "min    3.788800e+06  \n",
       "25%    8.779800e+06  \n",
       "50%    1.098290e+07  \n",
       "75%    1.461540e+07  \n",
       "max    5.125090e+07  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Stock Prices:\n",
    "\n",
    "- Handle missing values by forward-filling or removing them.\n",
    "- Normalize prices or returns to bring them into a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Returns  Normalized_Returns\n",
      "Date                                    \n",
      "2023-01-03  0.000000           -0.024436\n",
      "2023-01-04  0.022416            1.149628\n",
      "2023-01-05  0.001827            0.071263\n",
      "2023-01-06  0.040121            2.076976\n",
      "2023-01-09 -0.008767           -0.483610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_358030/2474532667.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)  # Forward-fill missing values\n"
     ]
    }
   ],
   "source": [
    "# Example: Cleaning and normalizing stock price data\n",
    "data.fillna(method='ffill', inplace=True)  # Forward-fill missing values\n",
    "data['Returns'] = data['Adj Close'].pct_change().fillna(0)  # Calculate daily returns\n",
    "data['Normalized_Returns'] = (data['Returns'] - data['Returns'].mean()) / data['Returns'].std()\n",
    "\n",
    "print(data[['Returns', 'Normalized_Returns']].head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial News Articles: News articles related to the stock market, which can be sourced from news aggregators like Google News, or specific financial news providers like Bloomberg or Reuters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPE news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "def item_search(item, limit, page):\n",
    "    stocknews = f\"https://www.thestar.com.my/search/?q={item}&qsort=oldest&qrec={limit}&qstockcode=&pgno={page}\"\n",
    "\n",
    "    html = requests.get(stocknews).text\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_details(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    content = soup.find('div', {'id':'story-body'})\n",
    "    if content:\n",
    "        content.get_text(strip=True)\n",
    "        return content\n",
    "    else:\n",
    "        return \"\"\n",
    "def star_new_crawler(page, search_query, limit):\n",
    "\n",
    "    title = []\n",
    "    links = []\n",
    "    premium = []\n",
    "    new_type = []\n",
    "    contents = []\n",
    "    publishedDate = []\n",
    "    while True:\n",
    "        print(page)\n",
    "        try:\n",
    "            result = item_search(search_query, limit, page)\n",
    "            title += [x.get_text(strip=True) for x in result.find_all(\"h2\", {\"class\": \"f18\"})]\n",
    "            links += [x.find('a', {\"data-content-type\": \"Article\"})['href'] for x in\n",
    "                     result.find_all(\"h2\", {\"class\": \"f18\"})]\n",
    "            premium += [x.get_text(strip=True) for x in result.find_all(\"span\", {\"class\": \"biz-icon\"})]\n",
    "            new_type += [x.get_text(strip=True) for x in result.find_all(\"a\", {\"class\": \"kicker\"})]\n",
    "            contents += [get_details(x) for x in links]\n",
    "            publishedDate += [x.get_text(strip=True) for x in result.find_all(\"span\", {\"class\": \"timestamp\"})]\n",
    "         \n",
    "            if len(title) == 0:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            traceback.print_exc()\n",
    "    \n",
    "        page += 1\n",
    "\n",
    "    return pd.DataFrame({'new_type': new_type, 'title': title, 'premium': premium, 'links': links, 'published_data': publishedDate,\n",
    "                 'contents': contents}).to_excel(f'{search_query}.xlsx', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    page = 1\n",
    "    search_query = 'Hewlett Packard Enterprise'\n",
    "    limit = 30\n",
    "    df = star_new_crawler(page, search_query, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "The stock market should be great, not on a knife's edge. But that's the opportunity\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/27/patrolling-the-oceans-for-seafood-thieves-from-west-virginia.html on URL https://www.cnbc.com/video/2018/02/27/patrolling-the-oceans-for-seafood-thieves-from-west-virginia.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/a-freed-slave.html on URL https://www.cnbc.com/video/2018/02/05/a-freed-slave.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/canned-tuna-controversy.html on URL https://www.cnbc.com/video/2018/02/05/canned-tuna-controversy.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/checking-for-fraud-in-a-seafood-delivery.html on URL https://www.cnbc.com/video/2018/02/05/checking-for-fraud-in-a-seafood-delivery.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/13/sustainable-seafood-at-whole-foods.html on URL https://www.cnbc.com/video/2018/02/13/sustainable-seafood-at-whole-foods.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/suspected-child-labor-on-a-thai-fishing-boat.html on URL https://www.cnbc.com/video/2018/02/05/suspected-child-labor-on-a-thai-fishing-boat.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/a-typical-thai-fishing-boat.html on URL https://www.cnbc.com/video/2018/02/05/a-typical-thai-fishing-boat.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/nyc-takes-on-the-seafood-industry-with-poke.html on URL https://www.cnbc.com/video/2018/02/05/nyc-takes-on-the-seafood-industry-with-poke.html\n",
      "Failed to download article: Article `download()` failed with 500 Server Error: Internal Server Error for url: https://www.cnbc.com/video/2018/02/05/hawaiis-foreign-labor-force.html on URL https://www.cnbc.com/video/2018/02/05/hawaiis-foreign-labor-force.html\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "Failed to download article: can't compare offset-naive and offset-aware datetimes\n",
      "                                               title       date  \\\n",
      "0  The stock market should be great, not on a kni... 2024-08-11   \n",
      "\n",
      "                                                text  \\\n",
      "0  When I look at Thursday's rally, I am beginnin...   \n",
      "\n",
      "                             source  \n",
      "0  https://www.cnbc.com/technology/  \n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define news sources to scrape\n",
    "news_sources = news_sources = [\n",
    "    \"https://www.reuters.com/technology/\",\n",
    "    \"https://www.cnbc.com/technology/\",\n",
    "    \"https://www.bloomberg.com/technology\"\n",
    "    \"https://www.forbes.com/sites/technology/\"\n",
    "    \"https://www.businesstoday.in/latest/economy/\"\n",
    "]\n",
    "# Keywords to filter articles\n",
    "keywords = [\"HPE\", \"Hewlett Packard Enterprise\"]#,\"Moonshot\",\"GreenLake\",\"HPE for HPC\",\"Aruba\",\"HPE Ezmeral\",\"HPE Pointnext\",\"HPE Financial Services\"]\n",
    "\n",
    "# Date range for filtering\n",
    "start_date = datetime(2021, 8, 1)\n",
    "end_date = datetime(2024, 9, 1)\n",
    "\n",
    "# Function to collect articles\n",
    "def collect_articles(news_sources, keywords, start_date, end_date):\n",
    "    articles = []\n",
    "    \n",
    "    for source in news_sources:\n",
    "        paper = newspaper.build(source, memoize_articles=False)\n",
    "        \n",
    "        for article in paper.articles:\n",
    "            try:\n",
    "                article.download()\n",
    "                article.parse()\n",
    "\n",
    "                # Check if the article's publication date is within the desired range\n",
    "                if article.publish_date and start_date <= article.publish_date <= end_date:\n",
    "                    # Check if the article contains any of the keywords\n",
    "                    if any(keyword in article.text for keyword in keywords):\n",
    "                        print(article.title)\n",
    "                        articles.append({\n",
    "                            \"title\": article.title,\n",
    "                            \"date\": article.publish_date,\n",
    "                            \"text\": article.text,\n",
    "                            \"source\": source\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download article: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Collect articles\n",
    "articles = collect_articles(news_sources, keywords, start_date, end_date)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(articles)\n",
    "df.dropna(subset=['date'], inplace=True)  # Drop articles without a publish date\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "#df.to_csv('apple_microsoft_financial_news.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The stock market should be great, not on a kni...</td>\n",
       "      <td>2024-08-11</td>\n",
       "      <td>When I look at Thursday's rally, I am beginnin...</td>\n",
       "      <td>https://www.cnbc.com/technology/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title       date  \\\n",
       "0  The stock market should be great, not on a kni... 2024-08-11   \n",
       "\n",
       "                                                text  \\\n",
       "0  When I look at Thursday's rally, I am beginnin...   \n",
       "\n",
       "                             source  \n",
       "0  https://www.cnbc.com/technology/  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/hpe_financial_news.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin_ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e646c1d0f36d352194803eb1b3bd6a8438ff42a362f7da11dd92f9220eb2998e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
